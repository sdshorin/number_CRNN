{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9912c5f-dbfe-46b5-81d2-9ea3e4c2c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers_generator.py\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageOps, ImageChops\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class HandwrittenNumbersDataset(Dataset):\n",
    "    digits_per_class = 5000\n",
    "\n",
    "    def __init__(self, custom_dataset_folder, mnist_dataset, max_digits=5, length=100_000, include_leading_zeros=False, seed=None, pre_generate=False, num_threads=1):\n",
    "        self.custom_dataset_folder = custom_dataset_folder\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.max_digits = max_digits\n",
    "        self.include_leading_zeros = include_leading_zeros\n",
    "        self.seed = seed\n",
    "        self.pre_generate = pre_generate\n",
    "        self.num_threads = num_threads\n",
    "        self.length = length\n",
    "\n",
    "        # Load digit images\n",
    "        self.digit_images = self.load_digit_images()\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Pre-generate data if required\n",
    "        if self.pre_generate:\n",
    "            start_time = time.time()\n",
    "            self.data = []\n",
    "            self.generate_data_single_threaded()\n",
    "            end_time = time.time()\n",
    "            print(f\"Data generation time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def load_digit_images(self):\n",
    "        # Load images from custom dataset and MNIST\n",
    "        digit_images = {str(i): [] for i in range(10)}\n",
    "        custom_counts = {}\n",
    "\n",
    "        # Load custom dataset images\n",
    "        for digit in range(10):\n",
    "            folder_path = os.path.join(self.custom_dataset_folder, str(digit))\n",
    "            if os.path.exists(folder_path):\n",
    "                images = []\n",
    "                for img_file in os.listdir(folder_path):\n",
    "                    if img_file.endswith('.png'):\n",
    "                        img_path = os.path.join(folder_path, img_file)\n",
    "                        image = Image.open(img_path).convert('L')  # Grayscale\n",
    "                        # Invert image if necessary\n",
    "                        if np.mean(image) > 127:\n",
    "                            image = ImageOps.invert(image)\n",
    "                        images.append(image)\n",
    "                print(f\"use {folder_path}, load {len(images)} images\")\n",
    "                digit_images[str(digit)].extend(images)\n",
    "                custom_counts[str(digit)] = len(images)\n",
    "            else:\n",
    "                custom_counts[str(digit)] = 0\n",
    "\n",
    "        # Limit MNIST usage to complement up to 1000 digits per class\n",
    "        mnist_digit_counts = {str(i): 0 for i in range(10)}\n",
    "        mnist_digit_images = {str(i): [] for i in range(10)}\n",
    "        for img, label in self.mnist_dataset:\n",
    "            label_str = str(label)\n",
    "            if mnist_digit_counts[label_str] < self.digits_per_class - custom_counts[label_str]:\n",
    "                # Invert MNIST images if necessary\n",
    "                if np.mean(img) > 127:\n",
    "                    img = ImageOps.invert(img)\n",
    "                mnist_digit_images[label_str].append(img)\n",
    "                mnist_digit_counts[label_str] += 1\n",
    "            if all(count >= self.digits_per_class - custom_counts[str(i)] for i, count in mnist_digit_counts.items()):\n",
    "                break\n",
    "\n",
    "        # Combine custom and MNIST images\n",
    "        for digit in digit_images.keys():\n",
    "            digit_images[digit].extend(mnist_digit_images[digit])\n",
    "\n",
    "        return digit_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.pre_generate:\n",
    "            return self.data[idx]\n",
    "        else:\n",
    "            return self.generate_sample()\n",
    "\n",
    "    def generate_sample(self):\n",
    "        # Generate a random number\n",
    "        num_digits = random.randint(1, self.max_digits)\n",
    "        if self.include_leading_zeros and random.random() < 0.1:\n",
    "            # 10% chance to include leading zeros\n",
    "            number_str = ''.join([str(random.randint(0, 9)) for _ in range(num_digits)])\n",
    "            number_str = number_str.zfill(self.max_digits)\n",
    "        else:\n",
    "            number = random.randint(0, 10**num_digits - 1)\n",
    "            number_str = str(number).zfill(num_digits)\n",
    "\n",
    "        # Build the image by concatenating digit images with random spacing and vertical position\n",
    "        digit_images = []\n",
    "        for digit_char in number_str:\n",
    "            digit_image = random.choice(self.digit_images[digit_char])\n",
    "            digit_image = self.augment_digit(digit_image)\n",
    "            digit_images.append(digit_image)\n",
    "\n",
    "        composite_image = self.concatenate_digits(digit_images)\n",
    "        processed_image = self.process_image(composite_image)\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image_tensor = transforms.ToTensor()(processed_image)\n",
    "\n",
    "        # Prepare the label\n",
    "        label = number_str  # Now label is a string\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "    def augment_digit(self, image):\n",
    "        # Data augmentation transformations\n",
    "        transform_list = []\n",
    "\n",
    "        # Random rotation\n",
    "        rotation_degree = random.uniform(-10, 10)\n",
    "        image = image.rotate(rotation_degree, fillcolor=0)\n",
    "\n",
    "        # Random zoom\n",
    "        scale_factor = random.uniform(0.9, 1.1)\n",
    "        new_size = (int(image.size[0]*scale_factor), int(image.size[1]*scale_factor))\n",
    "        image = image.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "        # Random shift\n",
    "        max_dx = 4\n",
    "        max_dy = 4\n",
    "        dx = random.randint(-max_dx, max_dx)\n",
    "        dy = random.randint(-max_dy, max_dy)\n",
    "        image = ImageChops.offset(image, dx, dy)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def concatenate_digits(self, digit_images):\n",
    "        # Random vertical shifts and spacing\n",
    "        max_digit_height = max(img.size[1] for img in digit_images)\n",
    "        vertical_shifts = [random.randint(-5, 5) for _ in digit_images]\n",
    "        spacings = [random.randint(-12, 10) for _ in range(len(digit_images)-1)]\n",
    "\n",
    "        # Calculate total width\n",
    "        total_width = sum(img.size[0] for img in digit_images) + sum(spacings)\n",
    "        canvas_height = max_digit_height + 10  # Extra space for vertical shifts\n",
    "\n",
    "        new_image = Image.new('L', (total_width, canvas_height), color=0)  # Black background\n",
    "\n",
    "        x_offset = 0\n",
    "        for i, im in enumerate(digit_images):\n",
    "            y_offset = (canvas_height - im.size[1]) // 2 + vertical_shifts[i]\n",
    "            temp_image = Image.new('L', new_image.size, color=0)\n",
    "            temp_image.paste(im, (x_offset, y_offset))\n",
    "            new_image = ImageChops.lighter(new_image, temp_image)\n",
    "\n",
    "            if i < len(spacings):\n",
    "                x_offset += im.size[0] + spacings[i]\n",
    "            else:\n",
    "                x_offset += im.size[0]\n",
    "\n",
    "        return new_image\n",
    "\n",
    "    def process_image(self, image):\n",
    "        # Scale image so that the maximum height of the content is 30 pixels\n",
    "        max_content_height = 30\n",
    "        w_percent = (max_content_height / float(image.size[1]))\n",
    "        new_width = int((float(image.size[0]) * float(w_percent)))\n",
    "        image = image.resize((new_width, max_content_height), Image.LANCZOS)\n",
    "\n",
    "        # Add 1-pixel margins to make height 32 pixels\n",
    "        image = ImageOps.expand(image, border=(0, 1), fill=0)  # Black background\n",
    "\n",
    "        # Right-align and crop or pad if necessary\n",
    "        if image.size[0] > 128:\n",
    "            image = image.crop((image.size[0] - 128, 0, image.size[0], image.size[1]))\n",
    "        else:\n",
    "            # Add padding to the left\n",
    "            padding = 128 - image.size[0]\n",
    "            image = ImageOps.expand(image, border=(padding, 0, 0, 0), fill=0)  # Black background\n",
    "\n",
    "        return image\n",
    "\n",
    "    def visualize_sample(self, idx=None):\n",
    "        if idx is None:\n",
    "            image_tensor, label = self.generate_sample()\n",
    "        else:\n",
    "            image_tensor, label = self[idx]\n",
    "        image = transforms.ToPILImage()(image_tensor)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def generate_data_single_threaded(self):\n",
    "        data_length = self.__len__()\n",
    "        self.data = []\n",
    "\n",
    "        for i in range(data_length):\n",
    "            self.data.append(self.generate_sample())\n",
    "\n",
    "        # Shuffle the data\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "\n",
    "def test_dataset():\n",
    "    # Load MNIST dataset\n",
    "    mnist_dataset = MNIST(root='./data', train=True, download=True)\n",
    "\n",
    "    length=100_000\n",
    "    seed = random.randint(0, 9999999)\n",
    "    # seed = 2122224\n",
    "    print(f\"Seed: {seed}\")\n",
    "    # Create the custom dataset\n",
    "    dataset = HandwrittenNumbersDataset(\n",
    "        custom_dataset_folder='/Users/sergejsorin/work/math/lib/mnist_improve/local_data/sorted',\n",
    "        mnist_dataset=mnist_dataset,\n",
    "        max_digits=5,\n",
    "        length=length,\n",
    "        include_leading_zeros=True,\n",
    "        seed=seed,  # For reproducibility\n",
    "        pre_generate=True  # Set to True to pre-generate data\n",
    "    )\n",
    "\n",
    "    # Visualize a sample for debugging\n",
    "    # dataset.visualize_sample()\n",
    "\n",
    "    dataset.generate_sample()\n",
    "    # # Measure dataset generation speed\n",
    "    # start_time = time.time()\n",
    "    # for _ in range(100):\n",
    "    #     dataset.generate_sample()\n",
    "    # end_time = time.time()\n",
    "    # print(f\"Time to generate 100 samples: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # # Use DataLoader for batching\n",
    "    # data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    import os\n",
    "    import hashlib\n",
    "    from PIL import Image\n",
    "    import io\n",
    "\n",
    "    def image_to_hash(image):\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        return hashlib.md5(img_byte_arr.getvalue()).hexdigest()\n",
    "\n",
    "    def check_and_save_images(dataset, num_images=100000, save_interval=1000):\n",
    "        os.makedirs('raw_images', exist_ok=True)\n",
    "        os.makedirs('raw_duplicates', exist_ok=True)\n",
    "        image_hashes = {}\n",
    "        duplicate_count = 0\n",
    "\n",
    "        for i in range(num_images):\n",
    "            image_tensor, label = dataset[i]\n",
    "            image = transforms.ToPILImage()(image_tensor)\n",
    "            \n",
    "            image_hash = image_to_hash(image)\n",
    "            number = ''.join(map(str, label.numpy()))\n",
    "            \n",
    "            if image_hash in image_hashes:\n",
    "                duplicate_count += 1\n",
    "                # Save duplicate\n",
    "                dup_filename = f\"raw_duplicates/{image_hash}_{number}_dup{duplicate_count}.png\"\n",
    "                image.save(dup_filename)\n",
    "                # Save original for comparison\n",
    "                orig_filename = f\"raw_duplicates/{image_hash}_{number}_orig.png\"\n",
    "                orig_image, _ = dataset[image_hashes[image_hash]]\n",
    "                orig_image = transforms.ToPILImage()(orig_image)\n",
    "                orig_image.save(orig_filename)\n",
    "            else:\n",
    "                image_hashes[image_hash] = i\n",
    "                if i % save_interval == 0:\n",
    "                    filename = f\"raw_images/{image_hash}_{number}.png\"\n",
    "                    image.save(filename)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Processed {i} images. Duplicates so far: {duplicate_count}\")\n",
    "\n",
    "        print(f\"Total images processed: {num_images}\")\n",
    "        print(f\"Total unique images: {len(image_hashes)}\")\n",
    "        print(f\"Total duplicates: {duplicate_count}\")\n",
    "\n",
    "    # Run the check\n",
    "    check_and_save_images(dataset, length)\n",
    "\n",
    "is_in_cloud = torch.cuda.is_available()\n",
    "if __name__ == \"__main__\":\n",
    "    if not is_in_cloud:\n",
    "        test_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2754b390-c3aa-4ab1-a681-b82a5d8780f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "DATASET_SIZE = 120_000\n",
    "TEST_DATASET_SIZE = 3_000\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCH = 75\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class CRNN(nn.Module):\n",
    "#     def __init__(self, imgH=32, nc=1, nclass=12, nh=256, n_rnn=2, leakyRelu=False):\n",
    "#         super(CRNN, self).__init__()\n",
    "#         assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "#         ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "#         ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "#         ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "#         nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "#         cnn = nn.Sequential()\n",
    "\n",
    "#         def convRelu(i, batchNormalization=False):\n",
    "#             nIn = nc if i == 0 else nm[i - 1]\n",
    "#             nOut = nm[i]\n",
    "#             cnn.add_module('conv{0}'.format(i),\n",
    "#                            nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "#             if batchNormalization:\n",
    "#                 cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "#             if leakyRelu:\n",
    "#                 cnn.add_module('relu{0}'.format(i),\n",
    "#                                nn.LeakyReLU(0.2, inplace=True))\n",
    "#             else:\n",
    "#                 cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "#         convRelu(0)\n",
    "#         cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "#         convRelu(1)\n",
    "#         cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "#         convRelu(2, True)\n",
    "#         convRelu(3)\n",
    "#         cnn.add_module('pooling{0}'.format(2),\n",
    "#                        nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "#         convRelu(4, True)\n",
    "#         convRelu(5)\n",
    "#         cnn.add_module('pooling{0}'.format(3),\n",
    "#                        nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "#         convRelu(6, True)  # 512x1x16\n",
    "\n",
    "#         self.cnn = cnn\n",
    "#         self.rnn = nn.Sequential(\n",
    "#             BidirectionalLSTM(nm[-1], nh, nh),\n",
    "#             BidirectionalLSTM(nh, nh, nclass))\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # conv features\n",
    "#         conv = self.cnn(input)\n",
    "#         b, c, h, w = conv.size()\n",
    "#         # assert h == 1, \"the height of conv must be 1\"\n",
    "#         conv = conv.squeeze(2)\n",
    "#         conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "\n",
    "#         # rnn features\n",
    "#         output = self.rnn(conv)\n",
    "\n",
    "#         # add log_softmax to converge output\n",
    "#         output = F.log_softmax(output, dim=2)\n",
    "\n",
    "#         return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH=32, nc=1, nclass=12, nh=128, n_rnn=1, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [32, 64, 128, 128, 256, 256, 256]  # Уменьшенные количества каналов\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))\n",
    "        convRelu(6, True)\n",
    "\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(nm[-1], nh, nclass))  # Уменьшено количество рекуррентных слоёв\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        output = self.rnn(conv)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Константы\n",
    "CTC_BLANK = '<BLANK>'\n",
    "OOV_TOKEN = '<OOV>'\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, alphabet):\n",
    "        self.char_map = {CTC_BLANK: 0, OOV_TOKEN: 1}\n",
    "        for i, char in enumerate(alphabet):\n",
    "            self.char_map[char] = i + 2\n",
    "        self.rev_char_map = {v: k for k, v in self.char_map.items()}\n",
    "\n",
    "    def encode(self, word_list):\n",
    "        enc_words = []\n",
    "        for word in word_list:\n",
    "            enc_words.append([self.char_map.get(char, self.char_map[OOV_TOKEN]) for char in word])\n",
    "        return enc_words\n",
    "\n",
    "    def decode(self, enc_word_list, merge_repeated=True):\n",
    "        dec_words = []\n",
    "        for word in enc_word_list:\n",
    "            word_chars = ''\n",
    "            prev_char = None\n",
    "            for char_enc in word:\n",
    "                char = self.rev_char_map.get(char_enc, OOV_TOKEN)\n",
    "                if char != CTC_BLANK and char != OOV_TOKEN:\n",
    "                    if not (merge_repeated and char == prev_char):\n",
    "                        word_chars += char\n",
    "                prev_char = char\n",
    "            dec_words.append(word_chars)\n",
    "        return dec_words\n",
    "\n",
    "\n",
    "# Вспомогательные функции\n",
    "def train(model, criterion, optimizer, train_loader, device, tokenizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(\"Output shape:\", output.shape)  # Проверка размерности выхода модели\n",
    "\n",
    "        target_lengths = torch.tensor([len(t) for t in targets], dtype=torch.long)\n",
    "        targets = torch.cat([torch.tensor(t, dtype=torch.long) for t in targets]).to(device)\n",
    "        input_lengths = torch.full(size=(output.size(1),), fill_value=output.size(0), dtype=torch.long)\n",
    "\n",
    "        # print(\"Target shape:\", targets.shape)  # Проверка размерности целевых данных\n",
    "        # print(\"Input lengths:\", input_lengths)\n",
    "        # print(\"Target lengths:\", target_lengths)\n",
    "\n",
    "        loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # if batch_idx % 100 == 0:\n",
    "        #     print(f'Train Batch {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, tokenizer, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels_encoded in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            output = output.cpu()\n",
    "            preds = output.argmax(dim=2).transpose(1, 0)\n",
    "            preds = preds.tolist()\n",
    "            decoded_preds = tokenizer.decode(preds, merge_repeated=True)\n",
    "            decoded_labels = tokenizer.decode(labels_encoded, merge_repeated=False)\n",
    "            for pred, label in zip(decoded_preds, decoded_labels):\n",
    "                if pred == label:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    data = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    data = torch.stack(data, dim=0)\n",
    "    labels_encoded = tokenizer.encode(labels)\n",
    "    # print(\"Sample encoded label:\", labels_encoded[0])  # Проверка закодированной метки\n",
    "    return data, labels_encoded\n",
    "\n",
    "def train_model(model, tokenizer):\n",
    "    is_in_cloud = False\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        is_in_cloud = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Определение алфавита и создание токенизатора\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    # Загрузка датасета MNIST\n",
    "    mnist_train = MNIST(root='./data', train=True, download=True)\n",
    "    mnist_test = MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "    # Создание датасетов\n",
    "    # if not is_in_cloud:\n",
    "    #     from numbers_generator import HandwrittenNumbersDataset\n",
    "\n",
    "    custom_dataset_folder = '/Users/sergejsorin/work/math/lib/mnist_improve/local_data/sorted'\n",
    "    if not os.path.exists(custom_dataset_folder):\n",
    "        custom_dataset_folder = \"/home/user/sorted\"\n",
    "\n",
    "    train_dataset = HandwrittenNumbersDataset(\n",
    "        custom_dataset_folder=custom_dataset_folder,  # Путь к вашему кастомному датасету, если есть\n",
    "        mnist_dataset=mnist_train,\n",
    "        max_digits=5,\n",
    "        length=DATASET_SIZE,\n",
    "        include_leading_zeros=False,\n",
    "        seed=42,\n",
    "        pre_generate=False,\n",
    "        num_threads=1\n",
    "    )\n",
    "\n",
    "    test_dataset = HandwrittenNumbersDataset(\n",
    "        custom_dataset_folder='',\n",
    "        mnist_dataset=mnist_test,\n",
    "        max_digits=5,\n",
    "        length=TEST_DATASET_SIZE,\n",
    "        include_leading_zeros=False,\n",
    "        seed=43,\n",
    "        pre_generate=False,\n",
    "        num_threads=1\n",
    "    )\n",
    "\n",
    "    from functools import partial\n",
    "    collate_fn_with_tokenizer = partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "    # Создание DataLoader\n",
    "    batch_size = BATCH_SIZE\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn_with_tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn_with_tokenizer)\n",
    "\n",
    "\n",
    "    # Определение критерия и оптимизатора\n",
    "    criterion = nn.CTCLoss(blank=tokenizer.char_map[CTC_BLANK], zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"Start training\")\n",
    "    print(f\"Train params: dataset_size={DATASET_SIZE}, test_dataset_size={TEST_DATASET_SIZE}, batch_size={BATCH_SIZE}, num_epochs={NUM_EPOCH}\")\n",
    "\n",
    "    import time\n",
    "    # Цикл обучения\n",
    "    num_epochs = NUM_EPOCH\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_start = time.time()\n",
    "        train_loss = train(model, criterion, optimizer, train_loader, device, tokenizer)\n",
    "        train_end = time.time()\n",
    "        val_accuracy = evaluate(model, tokenizer, test_loader, device)\n",
    "        train_accuracy = evaluate(model, tokenizer, train_loader, device)\n",
    "        eval_end = time.time()\n",
    "        estimated_end_time = train_start + (eval_end - train_start) * (num_epochs - epoch)\n",
    "        human_readable_eta = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(estimated_end_time))\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Time: {train_end - train_start:.2f}, Eval Time: {eval_end - train_end:.2f}, ETA: {human_readable_eta}')\n",
    "\n",
    "    # Пример использования модели для предсказания\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Предположим, у нас есть одно тестовое изображение из тестового датасета\n",
    "        data_iter = iter(test_loader)\n",
    "        test_image, test_label = next(data_iter)\n",
    "        test_image = test_image[0].unsqueeze(0).to(device)\n",
    "        output = model(test_image)\n",
    "        pred = output.argmax(dim=2).transpose(1, 0)\n",
    "        decoded_pred = tokenizer.decode(pred.tolist(), merge_repeated=True)[0]\n",
    "        print(f\"Predicted number: {decoded_pred}\")\n",
    "        print(f\"Actual number: {test_label[0]}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "is_in_cloud = torch.cuda.is_available()\n",
    "if __name__ == \"__main__\":\n",
    "    if not is_in_cloud:\n",
    "        \n",
    "        alphabet = \"0123456789\"\n",
    "        tokenizer = Tokenizer(alphabet)\n",
    "\n",
    "        model = CRNN(imgH=32, nc=1, nclass=len(tokenizer.char_map), nh=256, n_rnn=2, leakyRelu=False)\n",
    "\n",
    "        trained_model = train_model(model, tokenizer)\n",
    "        os.makedirs('models2', exist_ok=True)\n",
    "        torch.save(trained_model.state_dict(), 'models2/crnn_model.pth')\n",
    "        # torch.save(trained_model.state_dict(), 'models/crnn_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40878f76-5edd-4a88-94e4-c95318990c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use /home/user/sorted/0, load 60 images\n",
      "use /home/user/sorted/1, load 294 images\n",
      "use /home/user/sorted/2, load 85 images\n",
      "use /home/user/sorted/3, load 90 images\n",
      "use /home/user/sorted/4, load 348 images\n",
      "use /home/user/sorted/5, load 113 images\n",
      "use /home/user/sorted/6, load 86 images\n",
      "use /home/user/sorted/7, load 412 images\n",
      "use /home/user/sorted/8, load 124 images\n",
      "use /home/user/sorted/9, load 240 images\n",
      "Start training\n",
      "Train params: dataset_size=120000, test_dataset_size=3000, batch_size=64, num_epochs=75\n",
      "Epoch 1, Train Loss: 0.8742, Validation Accuracy: 0.9103, Train Accuracy: 0.8944, Train Time: 23.08, Eval Time: 17.54, ETA: 2024-09-15 16:59:48\n",
      "Epoch 2, Train Loss: 0.0732, Validation Accuracy: 0.9457, Train Accuracy: 0.9418, Train Time: 23.06, Eval Time: 17.23, ETA: 2024-09-15 16:59:24\n",
      "Epoch 3, Train Loss: 0.0538, Validation Accuracy: 0.9597, Train Accuracy: 0.9613, Train Time: 23.06, Eval Time: 17.44, ETA: 2024-09-15 16:59:39\n",
      "Epoch 4, Train Loss: 0.0439, Validation Accuracy: 0.9630, Train Accuracy: 0.9647, Train Time: 23.12, Eval Time: 17.52, ETA: 2024-09-15 16:59:49\n",
      "Epoch 5, Train Loss: 0.0383, Validation Accuracy: 0.9600, Train Accuracy: 0.9678, Train Time: 22.95, Eval Time: 17.45, ETA: 2024-09-15 16:59:31\n",
      "Epoch 6, Train Loss: 0.0335, Validation Accuracy: 0.9700, Train Accuracy: 0.9734, Train Time: 23.08, Eval Time: 17.32, ETA: 2024-09-15 16:59:32\n",
      "Epoch 7, Train Loss: 0.0302, Validation Accuracy: 0.9713, Train Accuracy: 0.9764, Train Time: 22.90, Eval Time: 17.41, ETA: 2024-09-15 16:59:26\n",
      "Epoch 8, Train Loss: 0.0283, Validation Accuracy: 0.9630, Train Accuracy: 0.9752, Train Time: 22.96, Eval Time: 17.51, ETA: 2024-09-15 16:59:36\n",
      "Epoch 9, Train Loss: 0.0253, Validation Accuracy: 0.9683, Train Accuracy: 0.9792, Train Time: 22.94, Eval Time: 17.45, ETA: 2024-09-15 16:59:31\n",
      "Epoch 10, Train Loss: 0.0243, Validation Accuracy: 0.9657, Train Accuracy: 0.9743, Train Time: 23.05, Eval Time: 17.42, ETA: 2024-09-15 16:59:36\n",
      "Epoch 11, Train Loss: 0.0224, Validation Accuracy: 0.9750, Train Accuracy: 0.9783, Train Time: 22.93, Eval Time: 17.31, ETA: 2024-09-15 16:59:22\n",
      "Epoch 12, Train Loss: 0.0205, Validation Accuracy: 0.9687, Train Accuracy: 0.9752, Train Time: 22.94, Eval Time: 17.30, ETA: 2024-09-15 16:59:22\n",
      "Epoch 13, Train Loss: 0.0198, Validation Accuracy: 0.9757, Train Accuracy: 0.9828, Train Time: 22.96, Eval Time: 17.56, ETA: 2024-09-15 16:59:39\n",
      "Epoch 14, Train Loss: 0.0187, Validation Accuracy: 0.9713, Train Accuracy: 0.9835, Train Time: 22.91, Eval Time: 17.20, ETA: 2024-09-15 16:59:14\n",
      "Epoch 15, Train Loss: 0.0181, Validation Accuracy: 0.9753, Train Accuracy: 0.9815, Train Time: 22.96, Eval Time: 17.39, ETA: 2024-09-15 16:59:28\n",
      "Epoch 16, Train Loss: 0.0180, Validation Accuracy: 0.9700, Train Accuracy: 0.9844, Train Time: 23.02, Eval Time: 17.32, ETA: 2024-09-15 16:59:28\n",
      "Epoch 17, Train Loss: 0.0160, Validation Accuracy: 0.9733, Train Accuracy: 0.9853, Train Time: 22.96, Eval Time: 17.26, ETA: 2024-09-15 16:59:21\n",
      "Epoch 18, Train Loss: 0.0173, Validation Accuracy: 0.9803, Train Accuracy: 0.9847, Train Time: 22.99, Eval Time: 17.43, ETA: 2024-09-15 16:59:33\n",
      "Epoch 19, Train Loss: 0.0159, Validation Accuracy: 0.9807, Train Accuracy: 0.9865, Train Time: 22.96, Eval Time: 17.40, ETA: 2024-09-15 16:59:29\n",
      "Epoch 20, Train Loss: 0.0160, Validation Accuracy: 0.9700, Train Accuracy: 0.9868, Train Time: 22.96, Eval Time: 17.45, ETA: 2024-09-15 16:59:32\n",
      "Epoch 21, Train Loss: 0.0150, Validation Accuracy: 0.9787, Train Accuracy: 0.9879, Train Time: 23.14, Eval Time: 17.28, ETA: 2024-09-15 16:59:32\n",
      "Epoch 22, Train Loss: 0.0148, Validation Accuracy: 0.9800, Train Accuracy: 0.9870, Train Time: 23.08, Eval Time: 18.05, ETA: 2024-09-15 17:00:10\n",
      "Epoch 23, Train Loss: 0.0139, Validation Accuracy: 0.9767, Train Accuracy: 0.9882, Train Time: 23.05, Eval Time: 17.42, ETA: 2024-09-15 16:59:36\n",
      "Epoch 24, Train Loss: 0.0131, Validation Accuracy: 0.9770, Train Accuracy: 0.9884, Train Time: 22.99, Eval Time: 17.43, ETA: 2024-09-15 16:59:33\n",
      "Epoch 25, Train Loss: 0.0142, Validation Accuracy: 0.9680, Train Accuracy: 0.9868, Train Time: 22.99, Eval Time: 17.36, ETA: 2024-09-15 16:59:29\n",
      "Epoch 26, Train Loss: 0.0128, Validation Accuracy: 0.9723, Train Accuracy: 0.9856, Train Time: 23.04, Eval Time: 17.40, ETA: 2024-09-15 16:59:34\n",
      "Epoch 27, Train Loss: 0.0128, Validation Accuracy: 0.9760, Train Accuracy: 0.9893, Train Time: 22.94, Eval Time: 17.35, ETA: 2024-09-15 16:59:26\n",
      "Epoch 28, Train Loss: 0.0127, Validation Accuracy: 0.9803, Train Accuracy: 0.9886, Train Time: 23.01, Eval Time: 17.52, ETA: 2024-09-15 16:59:38\n",
      "Epoch 29, Train Loss: 0.0114, Validation Accuracy: 0.9767, Train Accuracy: 0.9885, Train Time: 22.90, Eval Time: 17.81, ETA: 2024-09-15 16:59:46\n",
      "Epoch 30, Train Loss: 0.0124, Validation Accuracy: 0.9737, Train Accuracy: 0.9866, Train Time: 22.98, Eval Time: 17.38, ETA: 2024-09-15 16:59:30\n",
      "Epoch 31, Train Loss: 0.0118, Validation Accuracy: 0.9797, Train Accuracy: 0.9901, Train Time: 23.06, Eval Time: 17.51, ETA: 2024-09-15 16:59:40\n",
      "Epoch 32, Train Loss: 0.0122, Validation Accuracy: 0.9770, Train Accuracy: 0.9887, Train Time: 23.09, Eval Time: 17.61, ETA: 2024-09-15 16:59:45\n",
      "Epoch 33, Train Loss: 0.0107, Validation Accuracy: 0.9720, Train Accuracy: 0.9890, Train Time: 22.97, Eval Time: 17.43, ETA: 2024-09-15 16:59:32\n",
      "Epoch 34, Train Loss: 0.0115, Validation Accuracy: 0.9797, Train Accuracy: 0.9909, Train Time: 23.21, Eval Time: 17.49, ETA: 2024-09-15 16:59:45\n",
      "Epoch 35, Train Loss: 0.0113, Validation Accuracy: 0.9773, Train Accuracy: 0.9904, Train Time: 23.00, Eval Time: 17.27, ETA: 2024-09-15 16:59:28\n",
      "Epoch 36, Train Loss: 0.0105, Validation Accuracy: 0.9703, Train Accuracy: 0.9881, Train Time: 23.01, Eval Time: 17.34, ETA: 2024-09-15 16:59:31\n",
      "Epoch 37, Train Loss: 0.0107, Validation Accuracy: 0.9777, Train Accuracy: 0.9911, Train Time: 22.97, Eval Time: 17.49, ETA: 2024-09-15 16:59:35\n",
      "Epoch 38, Train Loss: 0.0104, Validation Accuracy: 0.9767, Train Accuracy: 0.9902, Train Time: 23.42, Eval Time: 17.33, ETA: 2024-09-15 16:59:46\n",
      "Epoch 39, Train Loss: 0.0103, Validation Accuracy: 0.9790, Train Accuracy: 0.9910, Train Time: 23.02, Eval Time: 17.97, ETA: 2024-09-15 16:59:54\n",
      "Epoch 40, Train Loss: 0.0100, Validation Accuracy: 0.9753, Train Accuracy: 0.9912, Train Time: 23.07, Eval Time: 17.31, ETA: 2024-09-15 16:59:33\n",
      "Epoch 41, Train Loss: 0.0105, Validation Accuracy: 0.9770, Train Accuracy: 0.9914, Train Time: 23.00, Eval Time: 17.30, ETA: 2024-09-15 16:59:30\n",
      "Epoch 42, Train Loss: 0.0093, Validation Accuracy: 0.9827, Train Accuracy: 0.9915, Train Time: 23.09, Eval Time: 17.52, ETA: 2024-09-15 16:59:40\n",
      "Epoch 43, Train Loss: 0.0094, Validation Accuracy: 0.9713, Train Accuracy: 0.9882, Train Time: 23.08, Eval Time: 17.80, ETA: 2024-09-15 16:59:49\n",
      "Epoch 44, Train Loss: 0.0088, Validation Accuracy: 0.9730, Train Accuracy: 0.9893, Train Time: 23.02, Eval Time: 17.61, ETA: 2024-09-15 16:59:42\n",
      "Epoch 45, Train Loss: 0.0092, Validation Accuracy: 0.9783, Train Accuracy: 0.9890, Train Time: 23.27, Eval Time: 17.52, ETA: 2024-09-15 16:59:46\n",
      "Epoch 46, Train Loss: 0.0094, Validation Accuracy: 0.9773, Train Accuracy: 0.9922, Train Time: 23.03, Eval Time: 17.36, ETA: 2024-09-15 16:59:35\n",
      "Epoch 47, Train Loss: 0.0090, Validation Accuracy: 0.9753, Train Accuracy: 0.9900, Train Time: 23.06, Eval Time: 17.42, ETA: 2024-09-15 16:59:37\n",
      "Epoch 48, Train Loss: 0.0093, Validation Accuracy: 0.9753, Train Accuracy: 0.9908, Train Time: 23.06, Eval Time: 17.39, ETA: 2024-09-15 16:59:36\n",
      "Epoch 49, Train Loss: 0.0086, Validation Accuracy: 0.9727, Train Accuracy: 0.9908, Train Time: 23.04, Eval Time: 17.51, ETA: 2024-09-15 16:59:39\n",
      "Epoch 50, Train Loss: 0.0090, Validation Accuracy: 0.9780, Train Accuracy: 0.9920, Train Time: 23.00, Eval Time: 17.42, ETA: 2024-09-15 16:59:36\n",
      "Epoch 51, Train Loss: 0.0082, Validation Accuracy: 0.9830, Train Accuracy: 0.9912, Train Time: 23.00, Eval Time: 17.38, ETA: 2024-09-15 16:59:35\n",
      "Epoch 52, Train Loss: 0.0081, Validation Accuracy: 0.9723, Train Accuracy: 0.9876, Train Time: 23.01, Eval Time: 18.47, ETA: 2024-09-15 17:00:00\n",
      "Epoch 53, Train Loss: 0.0087, Validation Accuracy: 0.9787, Train Accuracy: 0.9917, Train Time: 23.39, Eval Time: 17.61, ETA: 2024-09-15 16:59:50\n",
      "Epoch 54, Train Loss: 0.0082, Validation Accuracy: 0.9793, Train Accuracy: 0.9916, Train Time: 22.96, Eval Time: 17.43, ETA: 2024-09-15 16:59:37\n",
      "Epoch 55, Train Loss: 0.0079, Validation Accuracy: 0.9807, Train Accuracy: 0.9932, Train Time: 23.07, Eval Time: 17.27, ETA: 2024-09-15 16:59:36\n",
      "Epoch 56, Train Loss: 0.0082, Validation Accuracy: 0.9787, Train Accuracy: 0.9916, Train Time: 23.11, Eval Time: 17.58, ETA: 2024-09-15 16:59:42\n",
      "Epoch 57, Train Loss: 0.0078, Validation Accuracy: 0.9780, Train Accuracy: 0.9917, Train Time: 22.96, Eval Time: 17.72, ETA: 2024-09-15 16:59:42\n",
      "Epoch 58, Train Loss: 0.0078, Validation Accuracy: 0.9747, Train Accuracy: 0.9915, Train Time: 22.96, Eval Time: 17.37, ETA: 2024-09-15 16:59:36\n",
      "Epoch 59, Train Loss: 0.0069, Validation Accuracy: 0.9743, Train Accuracy: 0.9918, Train Time: 22.94, Eval Time: 17.31, ETA: 2024-09-15 16:59:35\n",
      "Epoch 60, Train Loss: 0.0073, Validation Accuracy: 0.9850, Train Accuracy: 0.9907, Train Time: 23.02, Eval Time: 17.70, ETA: 2024-09-15 16:59:42\n",
      "Epoch 61, Train Loss: 0.0075, Validation Accuracy: 0.9763, Train Accuracy: 0.9931, Train Time: 23.00, Eval Time: 17.38, ETA: 2024-09-15 16:59:37\n",
      "Epoch 62, Train Loss: 0.0073, Validation Accuracy: 0.9843, Train Accuracy: 0.9926, Train Time: 23.07, Eval Time: 17.71, ETA: 2024-09-15 16:59:43\n",
      "Epoch 63, Train Loss: 0.0076, Validation Accuracy: 0.9753, Train Accuracy: 0.9932, Train Time: 23.04, Eval Time: 17.38, ETA: 2024-09-15 16:59:38\n",
      "Epoch 64, Train Loss: 0.0082, Validation Accuracy: 0.9793, Train Accuracy: 0.9939, Train Time: 23.20, Eval Time: 17.21, ETA: 2024-09-15 16:59:38\n",
      "Epoch 65, Train Loss: 0.0079, Validation Accuracy: 0.9783, Train Accuracy: 0.9919, Train Time: 22.94, Eval Time: 17.24, ETA: 2024-09-15 16:59:36\n",
      "Epoch 66, Train Loss: 0.0067, Validation Accuracy: 0.9830, Train Accuracy: 0.9932, Train Time: 23.01, Eval Time: 17.43, ETA: 2024-09-15 16:59:38\n",
      "Epoch 67, Train Loss: 0.0072, Validation Accuracy: 0.9783, Train Accuracy: 0.9917, Train Time: 23.00, Eval Time: 17.55, ETA: 2024-09-15 16:59:39\n",
      "Epoch 68, Train Loss: 0.0073, Validation Accuracy: 0.9790, Train Accuracy: 0.9940, Train Time: 23.20, Eval Time: 17.43, ETA: 2024-09-15 16:59:40\n",
      "Epoch 69, Train Loss: 0.0066, Validation Accuracy: 0.9787, Train Accuracy: 0.9921, Train Time: 23.04, Eval Time: 17.79, ETA: 2024-09-15 16:59:41\n",
      "Epoch 70, Train Loss: 0.0074, Validation Accuracy: 0.9757, Train Accuracy: 0.9928, Train Time: 23.02, Eval Time: 17.74, ETA: 2024-09-15 16:59:40\n",
      "Epoch 71, Train Loss: 0.0069, Validation Accuracy: 0.9777, Train Accuracy: 0.9927, Train Time: 22.94, Eval Time: 17.38, ETA: 2024-09-15 16:59:39\n",
      "Epoch 72, Train Loss: 0.0074, Validation Accuracy: 0.9773, Train Accuracy: 0.9936, Train Time: 22.95, Eval Time: 17.23, ETA: 2024-09-15 16:59:38\n",
      "Epoch 73, Train Loss: 0.0065, Validation Accuracy: 0.9810, Train Accuracy: 0.9936, Train Time: 23.14, Eval Time: 17.45, ETA: 2024-09-15 16:59:39\n",
      "Epoch 74, Train Loss: 0.0067, Validation Accuracy: 0.9820, Train Accuracy: 0.9930, Train Time: 23.06, Eval Time: 17.32, ETA: 2024-09-15 16:59:39\n",
      "Epoch 75, Train Loss: 0.0063, Validation Accuracy: 0.9783, Train Accuracy: 0.9938, Train Time: 23.37, Eval Time: 17.44, ETA: 2024-09-15 16:59:39\n",
      "Predicted number: 23230\n",
      "Actual number: [4, 5, 4, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "alphabet = \"0123456789\"\n",
    "tokenizer = Tokenizer(alphabet)\n",
    "\n",
    "model = CRNN(imgH=32, nc=1, nclass=len(tokenizer.char_map), nh=256, n_rnn=2, leakyRelu=False)\n",
    "\n",
    "trained_model = train_model(model, tokenizer)\n",
    "os.makedirs('models2', exist_ok=True)\n",
    "torch.save(trained_model.state_dict(), 'models2/crnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f401f736-111e-4cad-8920-29567a4648df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "def visualize_predictions(model, test_loader, device, tokenizer, num_samples=5):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Get a batch of test data\n",
    "            data_iter = iter(test_loader)\n",
    "            test_images, test_labels = next(data_iter)\n",
    "            \n",
    "            # Select the first image from the batch\n",
    "            test_image = test_images[0].unsqueeze(0).to(device)\n",
    "            test_label = test_labels[0]\n",
    "\n",
    "            test_label = [val for pair in zip(test_label, [0]*len(test_label)) for val in pair]\n",
    "            print(test_label)\n",
    "            decoded_test_label = tokenizer.decode([test_label], merge_repeated=True)[0]\n",
    "            \n",
    "            # Make prediction\n",
    "            output = model(test_image)\n",
    "            pred = output.argmax(dim=2).transpose(1, 0)\n",
    "            decoded_pred = tokenizer.decode(pred.tolist(), merge_repeated=True)[0]\n",
    "            \n",
    "            # Convert tensor to image\n",
    "            image = transforms.ToPILImage()(test_image.squeeze().cpu())\n",
    "            \n",
    "            # Display the image and predictions\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.title(f\"Predicted: {decoded_pred} | Actual: {decoded_test_label}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"Predicted number: {decoded_pred}\")\n",
    "            print(f\"Actual number: {decoded_test_label}\")\n",
    "            if decoded_pred != decoded_test_label:\n",
    "                print(\"ERROR\")\n",
    "            print()\n",
    "\n",
    "mnist_test = MNIST(root='./data', train=False, download=True)\n",
    "test_dataset = HandwrittenNumbersDataset(\n",
    "    custom_dataset_folder='',\n",
    "    mnist_dataset=mnist_test,\n",
    "    max_digits=5,\n",
    "    length=TEST_DATASET_SIZE,\n",
    "    include_leading_zeros=False,\n",
    "    seed=43,\n",
    "    pre_generate=False,\n",
    "    num_threads=1\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "collate_fn_with_tokenizer = partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "    # Создание DataLoader\n",
    "batch_size = BATCH_SIZE\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn_with_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1bb5cbb5-0bf5-4bb7-9735-eb10cad8f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 0, 9, 0, 11, 0, 2, 0, 9, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Predicted number: 47907\n",
      "Actual number: 47907\n",
      "\n",
      "[4, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2:\n",
      "Predicted number: 2\n",
      "Actual number: 2\n",
      "\n",
      "[6, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 3:\n",
      "Predicted number: 4\n",
      "Actual number: 4\n",
      "\n",
      "[5, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 4:\n",
      "Predicted number: 3\n",
      "Actual number: 3\n",
      "\n",
      "[6, 0, 10, 0, 2, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 5:\n",
      "Predicted number: 480\n",
      "Actual number: 480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "visualize_predictions(model, test_loader, device, tokenizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1f6f8ae-8e78-42f7-832c-37f9f1b0060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.onnx\n",
    "\n",
    "# # Переводим модель на CPU\n",
    "# model.cpu()\n",
    "\n",
    "# # Выполняем динамическую квантизацию\n",
    "# quantized_model = torch.quantization.quantize_dynamic(\n",
    "#     model, {nn.Linear, nn.LSTM}, dtype=torch.qint8\n",
    "# )\n",
    "\n",
    "# # Создаем пример входных данных\n",
    "# dummy_input = torch.randn(1, 1, 32, 128)  # Измените размеры по необходимости\n",
    "\n",
    "# # Экспортируем квантованную модель в формат ONNX\n",
    "# torch.onnx.export(\n",
    "#     quantized_model,\n",
    "#     dummy_input,\n",
    "#     \"model.onnx\",\n",
    "#     opset_version=12,\n",
    "#     input_names=['input'],\n",
    "#     output_names=['output'],\n",
    "#     dynamic_axes={\n",
    "#         'input': {0: 'batch_size', 3: 'width'},\n",
    "#         'output': {0: 'sequence_length', 1: 'batch_size'}\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Если после этого требуется перевести модель обратно на GPU:\n",
    "# model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25e3ea2f-f45b-4c42-889b-dcf1b30d6ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace=True)\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu5): ReLU(inplace=True)\n",
       "    (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu6): ReLU(inplace=True)\n",
       "  )\n",
       "  (rnn): Sequential(\n",
       "    (0): BidirectionalLSTM(\n",
       "      (rnn): LSTM(512, 256, bidirectional=True)\n",
       "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (1): BidirectionalLSTM(\n",
       "      (rnn): LSTM(256, 256, bidirectional=True)\n",
       "      (embedding): Linear(in_features=512, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda() # old\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae51d1ff-5d77-4a74-add1-0b60943e6323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace=True)\n",
       "    (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu5): ReLU(inplace=True)\n",
       "    (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu6): ReLU(inplace=True)\n",
       "  )\n",
       "  (rnn): Sequential(\n",
       "    (0): BidirectionalLSTM(\n",
       "      (rnn): LSTM(256, 256, bidirectional=True)\n",
       "      (embedding): Linear(in_features=512, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2ff0b98-ce27-4986-ae35-b97bca6d6da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6626/3695005590.py:125: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert h == 1, \"the height of conv must be 1\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model exported to models2/crnn_model.onnx\n",
      "Model was successfully exported and verified!\n"
     ]
    }
   ],
   "source": [
    "def export_to_onnx(model, file_path, input_shape):\n",
    "    # Установите модель в режим оценки\n",
    "    model.eval()\n",
    "    \n",
    "    # Создайте пример входных данных\n",
    "    dummy_input = torch.randn(input_shape, requires_grad=True)\n",
    "    \n",
    "    # Экспортируйте модель\n",
    "    torch.onnx.export(model,                     # модель, которую мы экспортируем\n",
    "                      dummy_input,               # пример входных данных\n",
    "                      file_path,                 # путь, куда сохранить модель\n",
    "                      export_params=True,        # сохранить веса обученной модели внутри файла ONNX\n",
    "                      opset_version=11,          # версия ONNX\n",
    "                      do_constant_folding=True,  # оптимизация: если `True`, будут вычислены константы\n",
    "                      input_names=['input'],     # имена входов модели\n",
    "                      output_names=['output'],   # имена выходов модели\n",
    "                      dynamic_axes={'input': {0: 'batch_size', 3: 'width'},    # переменные размерности\n",
    "                                    'output': {0: 'batch_size', 1: 'sequence'}})\n",
    "    print(f\"Model exported to {file_path}\")\n",
    "\n",
    "# Загрузите обученную модель\n",
    "model = CRNN(imgH=32, nc=1, nclass=len(tokenizer.char_map), nh=256, n_rnn=2, leakyRelu=False)\n",
    "model.load_state_dict(torch.load('models2/crnn_model.pth'))\n",
    "\n",
    "# Экспортируйте модель в ONNX\n",
    "export_to_onnx(model, 'models2/crnn_model.onnx', input_shape=(1, 1, 32, 128))\n",
    "\n",
    "# Проверка экспортированной модели\n",
    "import onnx\n",
    "\n",
    "# Загрузите модель ONNX\n",
    "onnx_model = onnx.load(\"models2/crnn_model.onnx\")\n",
    "\n",
    "# Проверьте, что модель имеет правильную структуру\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "print(\"Model was successfully exported and verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d15394b-154b-485a-950c-c17d459974d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in ./.local/lib/python3.10/site-packages (1.16.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/lib/python3/dist-packages (from onnx) (4.21.12)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/lib/python3/dist-packages (from onnx) (1.21.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/lib/python3/dist-packages (from onnxruntime) (1.12.1-git20200711.33e2d80-dfsg1-0.6)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from onnxruntime) (21.3)\n",
      "Collecting numpy>=1.21.6\n",
      "  Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/lib/python3/dist-packages (from onnxruntime) (1.9)\n",
      "Requirement already satisfied: protobuf in /usr/lib/python3/dist-packages (from onnxruntime) (4.21.12)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, humanfriendly, coloredlogs, onnxruntime\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script humanfriendly is installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script coloredlogs is installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script onnxruntime_test is installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 numpy-2.1.1 onnxruntime-1.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a4facae-fbca-400f-8915-83d38309a4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: models2/crnn_model_quantized.onnx\n",
      "Quantized model verified successfully\n",
      "Original model size: 9.34 MB\n",
      "Quantized model size: 2.37 MB\n",
      "Size reduction: 74.65%\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "def quantize_onnx_model(input_path, output_path):\n",
    "    # Загружаем модель ONNX\n",
    "    model = onnx.load(input_path)\n",
    "    \n",
    "    # Выполняем динамическую квантизацию\n",
    "    quantize_dynamic(\n",
    "        model_input=input_path,\n",
    "        model_output=output_path,\n",
    "        weight_type=QuantType.QUInt8  # Квантизация весов до 8-бит целых чисел без знака\n",
    "    )\n",
    "    \n",
    "    print(f\"Quantized model saved to: {output_path}\")\n",
    "    \n",
    "    # Загружаем и проверяем квантизованную модель\n",
    "    quantized_model = onnx.load(output_path)\n",
    "    onnx.checker.check_model(quantized_model)\n",
    "    print(\"Quantized model verified successfully\")\n",
    "\n",
    "    # Выводим информацию о размере файлов\n",
    "    import os\n",
    "    original_size = os.path.getsize(input_path) / (1024 * 1024)  # размер в МБ\n",
    "    quantized_size = os.path.getsize(output_path) / (1024 * 1024)  # размер в МБ\n",
    "    print(f\"Original model size: {original_size:.2f} MB\")\n",
    "    print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "    print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\")\n",
    "\n",
    "# Путь к оригинальной ONNX модели\n",
    "original_onnx_path = 'models2/crnn_model.onnx'\n",
    "\n",
    "# Путь для сохранения квантизованной модели\n",
    "quantized_onnx_path = 'models2/crnn_model_quantized.onnx'\n",
    "\n",
    "# Квантизация модели\n",
    "try:\n",
    "    quantize_onnx_model(original_onnx_path, quantized_onnx_path)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during quantization: {str(e)}\")\n",
    "    print(\"Please check if the input model is valid and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6950edc-fd58-438f-b504-c9b02fab0f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
